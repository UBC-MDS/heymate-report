---
title: "Menu Engineering Supercharged"
subtitle: "MDS Capstone Final Report"
author: Hankun Xiao, Yasmin Hassan, Jessie Zhang, Zhiwei Zhang
bibliography: references.bib
format:
  pdf:
    toc: true
    toc-depth: 2
    number-sections: true
    fig-cap-location: top
    keep-tex: true
    papersize: a4
    fontsize: 11pt
    geometry: margin=1in
---


## Introduction

This technical report serves as a behind-the-scenes narrative of our capstone project for Heymate!. It documents how we tackled technical challenges and highlights key decisions made during development and deployment. It also provides a reference for future development.


## User Requirement Collection and Solution Consulting

This capstone project with Heymate! differed from typical capstone projects. Usually, the project partner provides specific objectives for the deliverable as well as the dataset. In the project proposal from Heymate!, they expected our data output as a dashboard to improve customer retention. However, at the beginning of the project, we did not receive a useful dataset.

Given this situation, we had a two-hour meeting with the partner to understand their business needs and consult on potential solutions. We received clarification that the partner actually wanted a **menu recommendation model** ("the recommendation model") for their clients (restaurant owners). For example, one of Heymate!'s clients, a Chinese Restaurant in Vancouver, wanted recommendations on what items to add and what items to remove, following marketing trends.

Correspondingly, we agreed with the project partner and the project supervisor that we would shift the project from "Customer Retention Supercharged" to "**Menu Engineering Supercharged**," and we agreed upon the techinical deliverables: 

- A data cleaning module that can clean menu data and extract key features.
- A knowledge base (database table) with proper schema design and initial data.
- A recommendation algorithm.
- A visualization demo for reference.

## Preparing the Dataset

To build the recommendation model, we needed training data. We compared several datasets shared by the partner, our advisor, and our own research:

- **Dothub Dataset Shared by the Partner:** This dataset is 66.4 GB and consists of 6,479,347 rows, including restaurant names and addresses in the US @dolthub_menus. We didn't choose this dataset because it did not include restaurant menu data.
- **Yelp Dataset Shared by the Partner:** This dataset includes 6,990,280 reviews from 150,346 restaurants @yelp_open_dataset. While restaurant reviews could potentially be a good feature for building the model, it lacked menu data. This limitation made it unsuitable for direct use.
- **What's On The Menu? Dataset by New York Public Library:** This dataset includes about 45,000 menus from the 1840s to 2016, containing restaurant information and menu data @nypl_menu. However, the dataset is old and doesn't necessarily reflect the most up-to-date trends, so we didn't choose it.
- **Uber Eats Dataset ("the Uber Eats Data"):** This dataset includes over 63,000 restaurants and more than 5 million menus from Uber Eats in the US, collected in 2023 @ubereats_dataset. It contains restaurant names, review information, and individual menu items. Due to its recency and completeness, we agreed on this dataset with the partner.


## Dataset EDA and Feature Engineering

Given the nature of the data, there were missing values, spelling mistakes, variations, and language differences. We needed to apply methods to clean the data and perform necessary feature engineering before proceeding with the analysis.

For feature engineering, we aimed to extract **dish base** and **dish flavors** from the item name, category, and descriptions in the menu. Here is an example:

> **Original Menu Item:** "Classic Cheeseburger"  
> **Category:** "Classic Burgers"  
> **Description:** "Smashed beef patty with cheddar cheese and your choice of toppings and sauce."  
>   
> **Extracted Features:**  
> - **Dish Base:** Burger  
> - **Dish Flavors:** Cheese, Beef

We considered the following approaches:

- **Regular Expressions:** Regular expressions are effective for processing data with consistent patterns. However, in our data, each restaurant had its own preference for writing the menu, making this method inconsistent for universal application.
- **An Existing Word Embedding Model from HuggingFace, Specialized with Menu Corpus:** This method was suggested by Professor Verada, who specializes in machine learning within the UBC MDS teaching team. Unfortunately, the Uber Eats data contained information in different languages, and we couldn't find a suitable multi-language model from HuggingFace.
- **A Generalized Large Language Model (LLM):** This method involves using an external LLM to clean the data. We added specific instructions through prompt engineering on how the data should be cleaned and engineered. The processed data was then retrieved from the model. Additionally, the partner shared their organization's ChatGPT subscription with us and provided API tokens. After balancing cost and efficiency, we chose the **GPT-4o mini model**.

Here is a table comparing the different approaches:

| Approach                            | Pros                                                                 | Cons                                |
|-------------------------------------|----------------------------------------------------------------------|-------------------------------------|
| Regular Expressions                 | Fast to compute; Free                                                | Limited flexibility                 |
| <br> | <br> | <br> |
| Existing Word Embedding Model (HuggingFace) | Semantic understanding; Efficiency; Free                        | Multi-lingual challenge             |
| <br> | <br> | <br> |
| Generalized Large Language Model (LLM) | Highly flexible; Supporting Multi-lingual; Semantic understanding | Costly; Slow; API Reliance          |

## Research on the Recommendation Algorithm

We researched three common recommendation methods used in the industry: **Collaborative Filtering**, **Content-Based Filtering**, and **Popularity-Based Recommendation**.

- **Collaborative Filtering / Matrix Factorization:** This method is widely used in industry, including by companies like Netflix, to generate recommendations based on user-item interaction data (e.g., purchase history or user ratings) @gomezuribe2016netflix. However, in the Uber Eats dataset, we do not have access to such user interaction data, making this method infeasible for our use case.
- **Content-Based Filtering with Vector Embeddings:** This involves using a pre-trained large language model to convert menu items from human-readable text into numeric vectors. For example, "Fried Chicken" might be transformed into $[0.1, 0.5, 0.9]$, while "Dim Sum" could become $[0.2, 0.6, 0.1]$. These numeric representations allow us to quantify the similarities and differences between different cuisines, enabling further recommendations. Unfortunately, due to time and resource constraints, we were unable to deploy a large language model locally or fine-tune one.
- **Popularity-Based Recommendation:** Popularity is a widely accepted baseline metric in the industry, used by platforms like Yelp @chitalia2023yelp, and GitHub @sreekala2020popularity. For instance, GitHub calculates a popularity score based on the number of forks a repository receives @alrubaye2020github.
For Heymate!, we designed a custom formula that incorporates the **number of reviews**, **average rating of a restaurant**, and the **frequency of a cuisine** to calculate a popularity score. These data points are available in the Uber Eats dataset. Additionally, this method is easy to understand and communicate to restaurant owners. Given its advantages in both interpretability and feasibility, we chose to adopt a **Popularity-Based Recommendation** approach in our project.


## Techinical Implemetation

During development, the team utilized GitHub and Slack for communication and collaboration. We modularized each component for future improvement.

- [`visualization_demo.ipynb`](https://github.com/UBC-MDS/heymate-report/blob/readme-update/script/visualization_demo.ipynb): A notebook that visualizes the recommendation output.
- [`knowledge_base_update.ipynb`](https://github.com/UBC-MDS/heymate-report/blob/readme-update/script/knowledge_base_update.ipynb): A notebook manages the task of updating the knowledge base of menu data.
- [`archived_function_app.py`](https://github.com/UBC-MDS/heymate-report/blob/readme-update/script/archived_function_app.py): Stores deprecated Azure Function logic.
- [`batch_cleaning.py`](https://github.com/UBC-MDS/heymate-report/blob/readme-update/script/batch_cleaning.py): Cleans raw menu data in batches.
- [`credential_validation_test_unit.py`](https://github.com/UBC-MDS/heymate-report/blob/readme-update/script/credential_validation_test_unit.py): Unit tests for verifying database and API credentials.
- [`flask_deploy.py`](https://github.com/UBC-MDS/heymate-report/blob/readme-update/script/flask_deploy.py): Deploy the data cleaning module locally under the Flask framework.
- [`menu_recommender.py`](https://github.com/UBC-MDS/heymate-report/blob/readme-update/script/menu_recommender.py): Recommends menu items and return the output as a JSON object.
- [`popularity_score_calculator.py`](https://github.com/UBC-MDS/heymate-report/blob/readme-update/script/popularity_score_calculator.py): Calculates popularity scores for items.
- [`util_database_reader.py`](https://github.com/UBC-MDS/heymate-report/blob/readme-update/script/util_database_reader.py): Utility to read data from a database.
- [`util_database_uploader.py`](https://github.com/UBC-MDS/heymate-report/blob/readme-update/script/util_database_uploader.py): Utility to upload or write data to a database.
- [`util_llm_data_cleaner.py`](https://github.com/UBC-MDS/heymate-report/blob/readme-update/script/util_llm_data_cleaner.py): Utility to send and retrieve the data to ChatGPT’s API.
- [`util_task_logger.py`](https://github.com/UBC-MDS/heymate-report/blob/readme-update/script/util_task_logger.py): Logs tasks or events during execution for tracking/debugging.

## Deployment of the Data Cleaning Module

### Research and Decision Making

Due to the numerous amount of data (3 million rows), we decided to deploy the data cleaning module using a **distributed deployment approach**. This means multiple working instances would clean batches concurrently and run on cloud machines. We considered three approaches:

- **Google Cloud Function (part of Google Cloud Platform):** One team member had experience working with it in the past; however, the partner did not have a Google Cloud Subscription.
- **Flask Function on an EC2 instance on Amazon Web Services (AWS):** This framework was introduced in the Cloud Computing Course during the MDS program. However, the partner did not have an AWS Subscription.
- **Azure Function:** Azure Function is similar to Google Cloud Function, and working instances can be triggered via an HTTP web request. The partner had an Azure subscription and shared the credentials with us.

### Initial Deployment and Blockers

Given the cloud computing resources from the partner, we chose **Azure Function** as our primary deployment plan. To automate the deployment process, the team utilized the continuous deployment workflow from GitHub by adding the following flow:

```yaml
  - name: 'Deploy to Azure Functions'
    uses: Azure/functions-action@v1
    id: fa
    with:
      app-name: ${{ env.AZURE_FUNCTIONAPP_NAME }}
      package: script
      publish-profile: ${{ secrets.AZURE_FUNCTIONAPP_PUBLISH_PROFILE }}
      scm-do-build-during-deployment: true
      enable-oryx-build: true
```

Unfortunately, the deployment failed due to security setup issues. The partner had never used Azure Function before, and their engineers did not have the time to configure the proper security settings for us. We also did not want to risk compromising security, as the database stores sensitive business and customer information.

### Alternative Deployment

We decided to deploy the data cleaning module locally on our computers under the **Flask infrastructure**. We could still utilize the distributed deployment framework to accelerate the process. With the team's effort, we successfully cleaned:

- All of Heymate!'s internal menu database, including  over 6,000 entries.
- More than 30,000 entries in the Uber Eats dataset.

This allowed us to proceed further with the implementation of the recommendation alogorithm without blocking the project timeline.

## Onboarding Instruction and Suggestions for Future Development

The developer should first install and activate the Conda environment using the `environment.yml` file, and then set up the `credentials` folder. More detailed instructions can be found [here](https://github.com/UBC-MDS/heymate-report). Please note that the ChatGPT API token is **not required** unless the developer plans to load additional data into the knowledge base.

[`visualization_demo.ipynb`](https://github.com/UBC-MDS/heymate-report/blob/readme-update/script/visualization_demo.ipynb) includes detailed instructions on how to trigger recommendations and visualize the output. The developer can customize the visualization by adjusting the Altair code and exporting the result as an HTML object. If they would like to update the formula used for calculating the popularity score, changes can be made in the [`popularity_score_calculator.py`](https://github.com/UBC-MDS/heymate-report/blob/readme-update/script/popularity_score_calculator.py) script.

Currently, there are 30,000 data entries in the knowledge base, which already provide decent results. If the developer wishes to load more data, they can refer to the instructions in [`knowledge_base_update.ipynb`](https://github.com/UBC-MDS/heymate-report/blob/readme-update/script/knowledge_base_update.ipynb). To use a different LLM model, they can make the necessary changes in [`util_llm_data_cleaner.py`](https://github.com/UBC-MDS/heymate-report/blob/readme-update/script/util_llm_data_cleaner.py).


There are a few areas where this project could be extended:

- **Deployment of the Data Cleaning Module:** We currently deploy the code locally under the Flask framework. The code can be deployed as an Azure Function with proper setup. We provided `archived_function_app.py` as a reference code.
- **Diverse Dataset:** We currently ingest only one dataset, the Uber Eats dataset. Potentially, more datasets could be added to enrich the recommendation model.
- **Visualization Improvement:** We provided a demo visualization of the top recommended items @fig-viz-demo. When integrating the system into Heymate!'s restaurant management system, the developer should adjust the font and coloring to ensure that the visualization is coherent to the scheme.

![Visualization Demo”](../image/final-8-viz.png){#fig-viz-demo width="80%" fig-align="center"}

- **Save cost on the LLM usage**: Save cost on LLM usage: To save costs in the long run, Heymate! can consider using an in-house local LLM such as Ollama. This can significantly reduce the usage cost of ChatGPT.

{{< pagebreak >}}

## References